{{ template "chart.header" . }}
{{ template "chart.deprecationWarning" . }}

{{ template "chart.description" . }}

{{ template "chart.homepageLine" . }}

# Description

This chart deploys Prometheus Server in Agent mode configured by the `newrelic-prometheus-configurator`.

The solution is deployed as a StatefulSet for sharding proposes.
Each Pod will execute the `newrelic-prometheus-configurator` init container which will convert the provided config to a config file in the Prometheus format. Once the init container finishes and saves the config in a shared volume, the container running Prometheus in Agent mode will start.

```mermaid
graph LR
  subgraph pod[Pod]
    direction TB
    subgraph volume[shared volume]
      plain[Prometheus Config]
    end

    subgraph init-container[init Container]
      configurator[Configurator] --> plain[Prometheus Config]
    end

    subgraph container[Main Container]
      plain[Prometheus Config] --> prom-agent[Prometheus-Agent]
    end

  end

  subgraph configMap
    NewRelic-Config   --> configurator[Configurator]
  end

classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef pod fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class configurator,init-container,container,prom-agent k8s;
class volume plain;
class pod pod;

```

# Helm installation

You can install this chart using [`nri-bundle`](https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle) located in the
[helm-charts repository](https://github.com/newrelic/helm-charts) or directly from this repository by adding this Helm repository:

```shell
helm repo add newrelic-prometheus https://newrelic.github.io/newrelic-prometheus-configurator
helm upgrade --install newrelic newrelic-prometheus/newrelic-prometheus-agent -f your-custom-values.yaml
```

{{ template "chart.sourcesSection" . }}

## Values managed globally

This chart implements the [New Relic's common Helm library](https://github.com/newrelic/helm-charts/tree/master/library/common-library) which
means that it honors a wide range of defaults and globals common to most New Relic Helm charts.

Options that can be defined globally include `affinity`, `nodeSelector`, `tolerations`, `proxy` and others. The full list can be found at
[user's guide of the common library](https://github.com/newrelic/helm-charts/blob/master/library/common-library/README.md).

## Chart particularities

### Configuration 

The configuration used is similar to the [Prometheus configuration](https://prometheus.io/docs/prometheus/latest/configuration/configuration/), but it includes some syntactic sugar to make easy to set up some special use-cases like Kubernetes targets, sharding and some New Relic related settings like remote write endpoints.

The configurator will create [scrape_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config), [relabel_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config), [remote_write](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write) and other entries based on the defined configuration.

As general rules:
- Configs parameters having the same name as the [Prometheus configuration](https://prometheus.io/docs/prometheus/latest/configuration/configuration/) should have similar behavior. For example, the `tls_config` defined inside a `Kubernetes.jobs` will have the same definition as [tls_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tls_config) of Prometheus and will affect all targets scraped by that job.
- Configs starting with `extra_` prefix will be appended to the ones created by the Configurator. For example, the relabel configs defined in `extra_relabel_config` on the Kubernetes section will be appended to the end of the list that is already being generated by the Configurator for filtering, sharding, metadata decoration, etc.

### Default Kubernetes jobs configuration

By default, some Kubernetes objects are discovered and scraped by Prometheus. Taking into account the snippet from `values.yaml` below:

```yaml
config:
  kubernetes:
    jobs:
    - job_name_prefix: kubernetes-job
      target_discovery:
        pod: true
        endpoints: true
        filter:
          annotations:
            prometheus.io/scrape: true
```

All pod and endpoints with the `prometheus.io/scrape: true` annotation will be scraped by default.

Check the [docs](https://github.com/newrelic/newrelic-prometheus-configurator/blob/main/docs/KubernetesTargetFilter.md) for more information about it usage.

### Kubernetes job examples

#### API Server metrics
By default, the API Server Service named `kubernetes` is created in the `default` namespace. The following configuration will scrape metrics from all endpoints behind the mentioned service using the Prometheus Pod bearer token as Authorization Header:

```yaml
config:
  kubernetes:
    jobs:
    - job_name_prefix: apiserver
      target_discovery:
        endpoints: true
      extra_relabel_config:
      # Filter endpoints on `default` namespace associated to `kubernetes` service.
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name]
        action: keep
        regex: default;kubernetes
     
      scheme: https
      tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
      authorization:
        credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
```

### Metrics Filtering

Check [docs](https://github.com/newrelic/newrelic-prometheus-configurator/blob/main/docs/MetricsFilters.md) for a detailed explanation and examples of how to filter metrics and labels.

### Self metrics

By default, it is defined as a job in `static_target.jobs` to obtain self-metrics. Particularly, a snippet like the one
below is used. If you define your own static_targets jobs, it is important to also include this kind of job in order
to keep getting self-metrics.

```yaml
config:
  static_targets:
    jobs:
    - job_name: self-metrics
      targets:
        - "localhost:9090"
      extra_metric_relabel_config:
        - source_labels: [__name__]
          regex: "<metrics to include defined in values.yaml>"
          action: keep
```

### Low data mode

There are two mechanisms to reduce the amount of data that this integration sends to New Relic. See this snippet from the `values.yaml` file:
```yaml
lowDataMode: false

config:
  common:
    scrape_interval: 30s
```

You might set `lowDataMode` flag to `true` (it will filter some metrics which can also be collected using New Relic Kubernetes integration), check
`values.yaml` for details.

It is also possible to adjust how frequently Prometheus scrapes the targets by setting up the` config.common.scrape_interval` value.


### Affinities and tolerations

The New Relic common library allows you to set affinities, tolerations, and node selectors globally using e.g. `.global.affinity` to ease the configuration
when you use this chart using `nri-bundle`. This chart has an extra level of granularity to the components that it deploys:
control plane, ksm, and kubelet.

Take this snippet as an example:
```yaml
global:
  affinity: {}
affinity: {}
```

The order to set the affinity is to set `affinity` field (at root level), if that value is empty, the chart fallbacks to `global.affinity`.

{{ template "chart.valuesSection" . }}

{{ if .Maintainers }}
## Maintainers
{{ range .Maintainers }}
{{- if .Name }}
{{- if .Url }}
* [{{ .Name }}]({{ .Url }})
{{- else }}
* {{ .Name }}
{{- end }}
{{- end }}
{{- end }}
{{- end }}
