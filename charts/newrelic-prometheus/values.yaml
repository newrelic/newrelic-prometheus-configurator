# -- Override the name of the chart
nameOverride: ""
# -- Override the full name of the release
fullnameOverride: ""

# -- Name of the Kubernetes cluster monitored. Can be configured also with `global.cluster`.
# Note it will be set as an external label in prometheus configuration, it will have precedence over `config.common.external_labels.cluster_name`
# and `customAttributes.cluster_name``.
cluster: ""
# -- This set this license key to use. Can be configured also with `global.licenseKey`
licenseKey: ""
# -- In case you don't want to have the license key in you values, this allows you to point to a user created secret to get the key from there. Can be configured also with `global.customSecretName`
customSecretName: ""
# -- In case you don't want to have the license key in you values, this allows you to point to which secret key is the license key located. Can be configured also with `global.customSecretLicenseKey`
customSecretLicenseKey: ""

# -- Adds extra attributes to prometheus external labels. Can be configured also with `global.customAttributes`. Please note, values defined
# in `common.config.externar_labels` will have precedence over `customAttributes`.
customAttributes: {}

# Images used by the chart for prometheus and New Relic configurator.
# @default See `values.yaml`
images:
  # -- The secrets that are needed to pull images from a custom registry.
  pullSecrets: []

  # -- Image for New Relic configurator.
  # @default See `values.yaml`
  configurator:
    registry: ""
    repository: newrelic/newrelic-prometheus-configurator
    pullPolicy: IfNotPresent
    # @default It defaults to `annotation.configuratorVersion` in `Chart.yaml`.
    tag: ""
  # -- Image for prometheus which is executed in agent mode.
  # @default See `values.yaml`
  prometheus:
    registry: ""
    repository: quay.io/prometheus/prometheus
    pullPolicy: IfNotPresent
    # @default It defaults to `appVersion` in `Chart.yaml`.
    tag: ""

# -- Settings controlling ServiceAccount creation.
# @default -- See `values.yaml`
serviceAccount:
  # -- Whether the chart should automatically create the ServiceAccount objects required to run.
  create: true
  annotations: {}
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# -- Additional labels for chart objects. Can be configured also with `global.labels`
labels: {}
# -- Annotations to be added to all pods created by the integration.
podAnnotations: {}
# -- Additional labels for chart pods. Can be configured also with `global.podLabels`
podLabels: {}

# -- Sets pod's priorityClassName. Can be configured also with `global.priorityClassName`
priorityClassName: ""
# -- (bool) Sets pod's hostNetwork. Can be configured also with `global.hostNetwork`
# @default -- `false`
hostNetwork:
# -- Sets security context (at pod level). Can be configured also with `global.podSecurityContext`
podSecurityContext: {}
# -- Sets security context (at container level). Can be configured also with `global.containerSecurityContext`
containerSecurityContext: {}

# -- Sets pod's dnsConfig. Can be configured also with `global.dnsConfig`
dnsConfig: {}

# -- Settings controlling RBAC objects creation.
rbac:
  # -- Whether the chart should automatically create the RBAC objects required to run.
  create: true
  # -- Whether the chart should create Pod Security Policy objects.
  pspEnabled: false

# -- Sets pod/node affinities set almost globally. (See [Affinities and tolerations](README.md#affinities-and-tolerations))
affinity: {}
# -- Sets pod's node selector almost globally. (See [Affinities and tolerations](README.md#affinities-and-tolerations))
nodeSelector: {}
# -- Sets pod's tolerations to node taints almost globally. (See [Affinities and tolerations](README.md#affinities-and-tolerations))
tolerations: []

# -- (bool) Send the metrics to the staging backend. Requires a valid staging license key. Can be configured also with `global.nrStaging`
# @default -- `false`
nrStaging:

# -- (bool) Reduces number of metrics sent in order to reduce costs. It can be configured also with `global.lowDataMode`.
# Specifically, it makes prometheus stop reporting some kubernetes cluster specific metrics, you can see details in `static/lowdatamodedefaults.yaml`.
# @default -- false
lowDataMode:

# -- It holds the New Relic prometheus configuration. Check `values.yaml` for details.
# @default -- It sets self metrics and target scraping for pod and endpoints kubernetes objects which include a specific annotation. Check `values.yaml` for details.
config:
  # -- Include global configuration for prometheus agent. See `values.yaml` for details.
  common:
#    # The labels to add to any timeseries that this Prometheus instance scrapes.
#    external_labels:
#      one: two
#   # How frequently to scrape targets by default.
    scrape_interval: 30s
#    # The default timeout when scraping targets.
#    scrape_timeout: 10s

  # -- (object) Newrelic remote-write configuration settings. It should work with the default value but if you need to
  # set it up you can customize most [prometheus remote_write](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write)
  # values, as described in `values.yaml`.
  newrelic_remote_write:
#    # Includes additional relabel configs for the New Relic remote write. Please note, this field is not parsed so it should
#    # include valid [prometheus write_relabel_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write)
#    extra_write_relabel_configs: []
#    proxy_url: ""
#    remote_timeout: 30s
#    # prometheus tls_config setup, see <https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tls_config> for details.
#    tls_config:
#    # queue_config setup, see prometheus remote-write tuning <https://prometheus.io/docs/practices/remote_write/#remote-write-tuning>
#    # for a more detailed explanation.
#    queue_config:
#       # Number of samples to buffer per shard before we block reading of more
#       # samples from the WAL. It is recommended to have enough capacity in each
#       # shard to buffer several requests to keep throughput up while processing
#       # occasional slow remote requests.
#       capacity: 2500
#       # Maximum number of shards, i.e. amount of concurrency.
#       max_shards: 200
#       # Minimum number of shards, i.e. amount of concurrency.
#       min_shards: 1
#       # Maximum number of samples per send.
#       max_samples_per_send: 500
#       # Maximum time a sample will wait in buffer.
#       batch_send_deadline: 5s
#       # Initial retry delay. Gets doubled for every retry.
#       min_backoff: 30ms
#       # Maximum retry delay.
#       max_backoff: 5s
#       # Retry upon receiving a 429 status code from the remote-write storage.
#       retry_on_http_429: false

  # -- (object) It includes additional remote-write configuration. Note this configuration is not parsed, so valid
  # [prometheus remote_write configuration](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write)
  # should be provided.
  extra_remote_write:

  # -- It allows defining scrape jobs for kubernetes in a simple way.
  # @default -- By default it defines jobs to scrape all pod and endpoints objects with `"prometheus.io/scrape"` annotation.
  kubernetes:
    jobs:
    # By default all pods and endpoints with `prometeus.io/scrape: true` annotation will be discovered.
    # The job_name_prefix is mandatory and no other job using the same prefix should be used in the whole configuration.
    - job_name_prefix: kubernetes-job
      # Target discovery field allows customizing how kubernetes discovery works.
      target_discovery:
        # -- Whether pods should be discovered.
        pod: true
        # -- Whether endpoints should be discovered.
        endpoints: true
        # -- Define filtering criteria, it is possible to set labels and/or annotations. All filters will apply (defined
        # filters are taken into account as an "and operation").
        filter:
          annotations:
            prometheus.io/scrape: true
          # labels: {}

  # -- For more info about static_targets, see `values.yaml` and
  # [scrape_config prometheus documentation](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config).
  # Please note, if you define `static_target.jobs` and still need self metrics you should also include the job to do so
  # (as defined by default).
  # @default -- Includes a self-metrics job, for more info see `values.yaml`.
  static_targets:
    # @ignore
    jobs:
#    # The job name assigned to scraped metrics by default.
#    - job_name: ""
#      # The targets specified for this job
#      targets: []
#      # Labels assigned to all metrics scraped from the targets.
#      labels: {}
#      # The HTTP resource path on which to fetch metrics from targets.
#      metrics_path: /metrics
#      # honor_labels controls how Prometheus handles conflicts between label,
#      # more info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
#      honor_labels: true
#      # honor_timestamps controls whether Prometheus respects the timestamps present n scraped data.
#      honor_timestamps: true
#      # Optional HTTP URL parameters.
#      params: {}
#      # Configures the protocol scheme used for requests.
#      scheme: "http"
#      # An uncompressed response body larger than this many bytes will cause the scrape to fail. 0 means no limit.
#      # Example: 100MB.
#      body_size_limit: 0
#      # Per-scrape limit on number of scraped samples that will be accepted. 0 means no limit.
#      sample_limit: 0
#      # Per-scrape config limit on number of unique targets that will be accepted. 0 means no limit.
#      target_limit: 0
#      # Per-scrape limit on number of labels that will be accepted for a sample. 0 means no limit.
#      label_limit: 0
#      # Per-scrape limit on length of labels name that will be accepted for a sample. 0 means no limit.
#      label_name_length_limit: 0
#      # Per-scrape limit on length of labels value that will be accepted for a sample. 0 means no limit
#      label_value_length_limit: 0
#      # How frequently to scrape targets from this job.
#      scrape_interval: 30s
#      # Per-scrape timeout when scraping this job.
#      scrape_timeout: 30s
#      # Configures the scrape request's TLS settings.
#      tls_config:
#      # Sets the `Authorization` header on every scrape request
#      authorization:
#      # List of target relabel configurations.
#      extra_relabel_config: []
#      # List of metric relabel configurations.
#      extra_metric_relabel_config: []
    # @ignore
    - job_name: self-metrics
      targets:
        - "localhost:9090"
      extra_metric_relabel_config:
        - source_labels: [__name__]
          regex: "\
            prometheus_agent_active_series|\
            prometheus_target_interval_length_seconds|\
            prometheus_target_scrape_pool_targets|\
            prometheus_remote_storage_samples_pending|\
            prometheus_remote_storage_samples_in_total|\
            prometheus_remote_storage_samples_retried_total|\
            prometheus_agent_corruptions_total|\
            prometheus_remote_storage_shards|\
            prometheus_sd_kubernetes_events_total|\
            prometheus_agent_checkpoint_creations_failed_total|\
            prometheus_agent_checkpoint_deletions_failed_total|\
            prometheus_remote_storage_samples_dropped_total|\
            prometheus_remote_storage_samples_failed_total|\
            prometheus_sd_kubernetes_http_request_total|\
            prometheus_agent_truncate_duration_seconds_sum|\
            prometheus_build_info|\
            process_resident_memory_bytes|\
            process_virtual_memory_bytes|\
            process_cpu_seconds_total"
          action: keep
  # -- It is possible to include extra scrape configuration in [prometheus format](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config).
  # Please note, it should be valid prometheus configuration which will not be parsed by the chart.
  extra_scrape_configs: {}
